{
  
    
        "post0": {
            "title": "Predicting Heart Disease Using Maching Learning",
            "content": "The original source of the dataset is from Kaggle, which combines 5 heart datasets over 11 common features. However, this project uses a modified version of the dataset The original dataset can be found here . In this learning project, I followed along with notebooks created by Fares Sayah on Kaggle, and Karan Bhanot on Medium . Problem Definition . Given clinical parameters about a patient, can we predict whether or not they have heart disease? . Import Libraries . import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns !pip install panel hvplot import hvplot.pandas from scipy import stats import sklearn %matplotlib inline sns.set_style(&quot;whitegrid&quot;) plt.style.use(&quot;fivethirtyeight&quot;) . Gather and Prepare the Data . heart_disease = pd.read_csv(&quot;heart-disease.csv&quot;) . heart_disease.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 303 entries, 0 to 302 Data columns (total 14 columns): # Column Non-Null Count Dtype -- -- 0 age 303 non-null int64 1 sex 303 non-null int64 2 cp 303 non-null int64 3 trestbps 303 non-null int64 4 chol 303 non-null int64 5 fbs 303 non-null int64 6 restecg 303 non-null int64 7 thalach 303 non-null int64 8 exang 303 non-null int64 9 oldpeak 303 non-null float64 10 slope 303 non-null int64 11 ca 303 non-null int64 12 thal 303 non-null int64 13 target 303 non-null int64 dtypes: float64(1), int64(13) memory usage: 33.3 KB . Data Dictionary . age (age in years) | sex (1 = male; 0 = female) | cp (chest pain, based on scale of 0-3) 0: Typical angina | 1: Atypical angina (chest pain not related to heart) | 2: Non-anginal pain (usually esophageal spasms) | 3: Asymptomatic (chest pain with no signs of disease) | . | trestbps (resting systolic blood pressure, in mmHg) | chol (serum cholesterol in mg/dL) | fbs (fasting blood sugar; 0 = less than or equal to 126 mg/dL; 1 &gt; 126 mg/dL) | restecg (resting ECG) . 0: Nothing to note | 1: ST-T Wave abnormality | 2: Possible or definite left ventricular hypertrophy | . | thalach (maximum heart rate acheived on stress test, between 60-202) . | exang (exercise induced angina; 1 = yes; 0 = no) | oldpeak (amount of ST depression induced by exercise) | slope (slope of the peak exercise ST segment) 0: Upsloping | 1: Flatsloping | 2: Downsloping | . | ca (number of major vessels colored by flouroscopy, 0-3) | thal (thalium stress test) . 1-3: normal | 6: fixed defect | 7: reversible defect (no blood flow during exercise) | . | target (have heart disease = 1; no heart disease = 0) . | heart_disease.head() . age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal target . 0 63 | 1 | 3 | 145 | 233 | 1 | 0 | 150 | 0 | 2.3 | 0 | 0 | 1 | 1 | . 1 37 | 1 | 2 | 130 | 250 | 0 | 1 | 187 | 0 | 3.5 | 0 | 0 | 2 | 1 | . 2 41 | 0 | 1 | 130 | 204 | 0 | 0 | 172 | 0 | 1.4 | 2 | 0 | 2 | 1 | . 3 56 | 1 | 1 | 120 | 236 | 0 | 1 | 178 | 0 | 0.8 | 2 | 0 | 2 | 1 | . 4 57 | 0 | 0 | 120 | 354 | 0 | 1 | 163 | 1 | 0.6 | 2 | 0 | 2 | 1 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Exploratory Data Analysis (EDA) . What questions am I trying to solve? | What kind of data do I have, and how do I work with it? | What&#39;s missing from the data, and how to deal with it? | Where are the outliers and why are they important? | How can I add, change, or remove features to get more out of the data? | . heart_disease.shape . (303, 14) . pd.set_option(&quot;display.float&quot;, &quot;{:.2f}&quot;.format) heart_disease.describe() . age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal target . count 303.00 | 303.00 | 303.00 | 303.00 | 303.00 | 303.00 | 303.00 | 303.00 | 303.00 | 303.00 | 303.00 | 303.00 | 303.00 | 303.00 | . mean 54.37 | 0.68 | 0.97 | 131.62 | 246.26 | 0.15 | 0.53 | 149.65 | 0.33 | 1.04 | 1.40 | 0.73 | 2.31 | 0.54 | . std 9.08 | 0.47 | 1.03 | 17.54 | 51.83 | 0.36 | 0.53 | 22.91 | 0.47 | 1.16 | 0.62 | 1.02 | 0.61 | 0.50 | . min 29.00 | 0.00 | 0.00 | 94.00 | 126.00 | 0.00 | 0.00 | 71.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | . 25% 47.50 | 0.00 | 0.00 | 120.00 | 211.00 | 0.00 | 0.00 | 133.50 | 0.00 | 0.00 | 1.00 | 0.00 | 2.00 | 0.00 | . 50% 55.00 | 1.00 | 1.00 | 130.00 | 240.00 | 0.00 | 1.00 | 153.00 | 0.00 | 0.80 | 1.00 | 0.00 | 2.00 | 1.00 | . 75% 61.00 | 1.00 | 2.00 | 140.00 | 274.50 | 0.00 | 1.00 | 166.00 | 1.00 | 1.60 | 2.00 | 1.00 | 3.00 | 1.00 | . max 77.00 | 1.00 | 3.00 | 200.00 | 564.00 | 1.00 | 2.00 | 202.00 | 1.00 | 6.20 | 2.00 | 4.00 | 3.00 | 1.00 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; counts = heart_disease.target.value_counts() counts . 1 165 0 138 Name: target, dtype: int64 . p = sns.countplot(data=heart_disease, x=&quot;target&quot;) p.set_xlabel(&quot;Heart Disease&quot;) p.set_ylabel(&quot;Counts&quot;); . heart_disease.isna().sum() . age 0 sex 0 cp 0 trestbps 0 chol 0 fbs 0 restecg 0 thalach 0 exang 0 oldpeak 0 slope 0 ca 0 thal 0 target 0 dtype: int64 . Notes: . Looks like there are 165 people with heart disease, and 138 without, so the dataset is fairly balanced . | There are no null values . | . categorical_val = [] continuous_val = [] for column in heart_disease.columns: if len(heart_disease[column].unique()) &lt;= 10: categorical_val.append(column) else: continuous_val.append(column) categorical_val . [&#39;sex&#39;, &#39;cp&#39;, &#39;fbs&#39;, &#39;restecg&#39;, &#39;exang&#39;, &#39;slope&#39;, &#39;ca&#39;, &#39;thal&#39;, &#39;target&#39;] . continuous_val . [&#39;age&#39;, &#39;trestbps&#39;, &#39;chol&#39;, &#39;thalach&#39;, &#39;oldpeak&#39;] . p = sns.countplot(data=heart_disease, x=&quot;target&quot;, hue=&quot;sex&quot;) p.set_xlabel(&quot;Heart Disease&quot;) plt.legend(bbox_to_anchor=(1.05,1), labels=[&#39;Female&#39;, &#39;Male&#39;]); . p = sns.countplot(data=heart_disease, x=&quot;cp&quot;, hue=&quot;target&quot;) p.set_xlabel(&quot;Chest Pain&quot;) plt.legend(labels=[&quot;No heart disease&quot;, &quot;Heart Disease&quot;]); . Chest Pain (Based on scale of 0-3) . 0: Typical angina | 1: Atypical angina (chest pain not related to heart) | 2: Non-anginal pain (usually esophageal spasms) | 3: Asymptomatic (chest pain with no signs of disease) | . Histograms of Continuous Data . plt.figure(figsize=(15,15)) for i, column in enumerate(continuous_val, 1): plt.subplot(3,2,i) sns.histplot(data=heart_disease, x=column, hue=&quot;target&quot;, multiple=&quot;stack&quot;) #plt.legend(labels=[&quot;No Ht Disease&quot;, &quot;Heart Disease&quot;]); . trestbps : resting blood pressure | chol : serum cholesterol (mg/dL) | thalach : maximum heart rate acheived on stress test (60-202) | oldpeak : ST depression induced by exercise relative to rest | . Target: . 0 (blue) = no heart disease | 1 (orange) = heart disease | . Scatterplot of Heart Disease in Relation to Age and Cholesterol . plt.figure(figsize=(9,7)) plt.scatter(heart_disease.age[heart_disease.target == 1], heart_disease.chol[heart_disease.target == 1], c=&quot;salmon&quot;) plt.scatter(heart_disease.age[heart_disease.target == 0], heart_disease.chol[heart_disease.target == 0], c=&quot;lightblue&quot;) plt.title(&quot;Correlation of Heart Disease with Age and Cholesterol&quot;) plt.xlabel(&quot;Age&quot;) plt.ylabel(&quot;Cholesterol&quot;) plt.legend([&quot;Disease&quot;, &quot;No Disease&quot;]); . There is no obvious correlation between cholesterol levels and heart disease! . Correlation Matrix . corr_matrix = heart_disease.corr() fig, ax = plt.subplots(figsize=(15,15)) ax = sns.heatmap(corr_matrix, annot = True, linewidths = 0.5, fmt = &quot;.2f&quot;, cmap = &quot;YlGnBu&quot;); bottom, top = ax.get_ylim() ax.set_ylim(bottom + 0.5, top - 0.5) . (14.5, -0.5) . The presence of chest pain and thalach (highest pulse rate acheived on stress test) seem to have the highest correlations with the target value. . Fasting blood sugar and cholesterol have the lowest correlation with the target variable. . Data Processing . Create dummy values | Scale values | . categorical_val.remove(&#39;target&#39;) dataset = pd.get_dummies(heart_disease, columns = categorical_val) . dataset.head() . age trestbps chol thalach oldpeak target sex_0 sex_1 cp_0 cp_1 ... slope_2 ca_0 ca_1 ca_2 ca_3 ca_4 thal_0 thal_1 thal_2 thal_3 . 0 63 | 145 | 233 | 150 | 2.30 | 1 | 0 | 1 | 0 | 0 | ... | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | . 1 37 | 130 | 250 | 187 | 3.50 | 1 | 0 | 1 | 0 | 0 | ... | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | . 2 41 | 130 | 204 | 172 | 1.40 | 1 | 1 | 0 | 0 | 1 | ... | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | . 3 56 | 120 | 236 | 178 | 0.80 | 1 | 0 | 1 | 0 | 1 | ... | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | . 4 57 | 120 | 354 | 163 | 0.60 | 1 | 1 | 0 | 1 | 0 | ... | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | . 5 rows × 31 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; print(heart_disease.columns) print(dataset.columns) . Index([&#39;age&#39;, &#39;sex&#39;, &#39;cp&#39;, &#39;trestbps&#39;, &#39;chol&#39;, &#39;fbs&#39;, &#39;restecg&#39;, &#39;thalach&#39;, &#39;exang&#39;, &#39;oldpeak&#39;, &#39;slope&#39;, &#39;ca&#39;, &#39;thal&#39;, &#39;target&#39;], dtype=&#39;object&#39;) Index([&#39;age&#39;, &#39;trestbps&#39;, &#39;chol&#39;, &#39;thalach&#39;, &#39;oldpeak&#39;, &#39;target&#39;, &#39;sex_0&#39;, &#39;sex_1&#39;, &#39;cp_0&#39;, &#39;cp_1&#39;, &#39;cp_2&#39;, &#39;cp_3&#39;, &#39;fbs_0&#39;, &#39;fbs_1&#39;, &#39;restecg_0&#39;, &#39;restecg_1&#39;, &#39;restecg_2&#39;, &#39;exang_0&#39;, &#39;exang_1&#39;, &#39;slope_0&#39;, &#39;slope_1&#39;, &#39;slope_2&#39;, &#39;ca_0&#39;, &#39;ca_1&#39;, &#39;ca_2&#39;, &#39;ca_3&#39;, &#39;ca_4&#39;, &#39;thal_0&#39;, &#39;thal_1&#39;, &#39;thal_2&#39;, &#39;thal_3&#39;], dtype=&#39;object&#39;) . from sklearn.preprocessing import StandardScaler s_sc = StandardScaler() col_to_scale = [&#39;age&#39;, &#39;trestbps&#39;, &#39;chol&#39;, &#39;thalach&#39;, &#39;oldpeak&#39;] dataset[col_to_scale] = s_sc.fit_transform(dataset[col_to_scale]) . dataset.head() . age trestbps chol thalach oldpeak target sex_0 sex_1 cp_0 cp_1 ... slope_2 ca_0 ca_1 ca_2 ca_3 ca_4 thal_0 thal_1 thal_2 thal_3 . 0 0.95 | 0.76 | -0.26 | 0.02 | 1.09 | 1 | 0 | 1 | 0 | 0 | ... | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | . 1 -1.92 | -0.09 | 0.07 | 1.63 | 2.12 | 1 | 0 | 1 | 0 | 0 | ... | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | . 2 -1.47 | -0.09 | -0.82 | 0.98 | 0.31 | 1 | 1 | 0 | 0 | 1 | ... | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | . 3 0.18 | -0.66 | -0.20 | 1.24 | -0.21 | 1 | 0 | 1 | 0 | 1 | ... | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | . 4 0.29 | -0.66 | 2.08 | 0.58 | -0.38 | 1 | 1 | 0 | 1 | 0 | ... | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | . 5 rows × 31 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Build Models . Classification Report Template . from sklearn.metrics import accuracy_score, confusion_matrix, classification_report def print_score(clf, X_train, y_train, X_test, y_test, train=True): if train: pred = clf.predict(X_train) clf_report = pd.DataFrame(classification_report(y_train, pred, output_dict=True)) print(&quot;Train Result: n================================================&quot;) print(f&quot;Accuracy Score: {accuracy_score(y_train, pred) * 100:.2f}%&quot;) print(&quot;_______________________________________________&quot;) print(f&quot;CLASSIFICATION REPORT: n{clf_report}&quot;) print(&quot;_______________________________________________&quot;) print(f&quot;Confusion Matrix: n {confusion_matrix(y_train, pred)} n&quot;) elif train==False: pred = clf.predict(X_test) clf_report = pd.DataFrame(classification_report(y_test, pred, output_dict=True)) print(&quot;Test Result: n================================================&quot;) print(f&quot;Accuracy Score: {accuracy_score(y_test, pred) * 100:.2f}%&quot;) print(&quot;_______________________________________________&quot;) print(f&quot;CLASSIFICATION REPORT: n{clf_report}&quot;) print(&quot;_______________________________________________&quot;) print(f&quot;Confusion Matrix: n {confusion_matrix(y_test, pred)} n&quot;) . Split Data into Train &amp; Test Data . from sklearn.model_selection import train_test_split X = dataset.drop(&#39;target&#39;, axis=1) y = dataset.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) . Now that the data is split into training and test sets, I can build machine learning models . Train the data on the training set . Test on the test set . Here, I&#39;ll try 5 different machine learning models: . Logistic Regression | K-Nearest Neighbors Classifier | Support Vector machine | Decision Tree Classifier | Random Forest Classifier | . Model 1: Logistic Regression . from sklearn.linear_model import LogisticRegression lr_clf = LogisticRegression(solver=&#39;liblinear&#39;) lr_clf.fit(X_train, y_train) print_score(lr_clf, X_train, y_train, X_test, y_test, train=True) print_score(lr_clf, X_train, y_train, X_test, y_test, train=False) . Train Result: ================================================ Accuracy Score: 86.79% _______________________________________________ CLASSIFICATION REPORT: 0 1 accuracy macro avg weighted avg precision 0.88 0.86 0.87 0.87 0.87 recall 0.82 0.90 0.87 0.86 0.87 f1-score 0.85 0.88 0.87 0.87 0.87 support 97.00 115.00 0.87 212.00 212.00 _______________________________________________ Confusion Matrix: [[ 80 17] [ 11 104]] Test Result: ================================================ Accuracy Score: 86.81% _______________________________________________ CLASSIFICATION REPORT: 0 1 accuracy macro avg weighted avg precision 0.87 0.87 0.87 0.87 0.87 recall 0.83 0.90 0.87 0.86 0.87 f1-score 0.85 0.88 0.87 0.87 0.87 support 41.00 50.00 0.87 91.00 91.00 _______________________________________________ Confusion Matrix: [[34 7] [ 5 45]] . test_score = accuracy_score(y_test, lr_clf.predict(X_test)) * 100 train_score = accuracy_score(y_train, lr_clf.predict(X_train)) * 100 results_df = pd.DataFrame(data=[[&quot;Logistic Regression&quot;, train_score, test_score]], columns=[&#39;Model&#39;, &#39;Training Accuracy %&#39;, &#39;Testing Accuracy %&#39;]) results_df . Model Training Accuracy % Testing Accuracy % . 0 Logistic Regression | 86.79 | 86.81 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Model 2: K-Nearest Neighbors . from sklearn.neighbors import KNeighborsClassifier knn_clf = KNeighborsClassifier() knn_clf.fit(X_train, y_train) print_score(knn_clf, X_train, y_train, X_test, y_test, train=True) print_score(knn_clf, X_train, y_train, X_test, y_test, train=False) . Train Result: ================================================ Accuracy Score: 86.79% _______________________________________________ CLASSIFICATION REPORT: 0 1 accuracy macro avg weighted avg precision 0.86 0.87 0.87 0.87 0.87 recall 0.85 0.89 0.87 0.87 0.87 f1-score 0.85 0.88 0.87 0.87 0.87 support 97.00 115.00 0.87 212.00 212.00 _______________________________________________ Confusion Matrix: [[ 82 15] [ 13 102]] Test Result: ================================================ Accuracy Score: 86.81% _______________________________________________ CLASSIFICATION REPORT: 0 1 accuracy macro avg weighted avg precision 0.85 0.88 0.87 0.87 0.87 recall 0.85 0.88 0.87 0.87 0.87 f1-score 0.85 0.88 0.87 0.87 0.87 support 41.00 50.00 0.87 91.00 91.00 _______________________________________________ Confusion Matrix: [[35 6] [ 6 44]] . test_score = accuracy_score(y_test, knn_clf.predict(X_test)) * 100 train_score = accuracy_score(y_train, knn_clf.predict(X_train)) * 100 results_df_2 = pd.DataFrame(data=[[&quot;K-nearest neighbors&quot;, train_score, test_score]], columns=[&#39;Model&#39;, &#39;Training Accuracy %&#39;, &#39;Testing Accuracy %&#39;]) results_df = results_df.append(results_df_2, ignore_index=True) results_df . Model Training Accuracy % Testing Accuracy % . 0 Logistic Regression | 86.79 | 86.81 | . 1 K-nearest neighbors | 86.79 | 86.81 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Model 3: Support Vector Machine . from sklearn.svm import SVC svm_clf = SVC(kernel=&#39;rbf&#39;, gamma=0.1, C=1.0) svm_clf.fit(X_train, y_train) print_score(svm_clf, X_train, y_train, X_test, y_test, train=True) print_score(svm_clf, X_train, y_train, X_test, y_test, train=False) . Train Result: ================================================ Accuracy Score: 93.40% _______________________________________________ CLASSIFICATION REPORT: 0 1 accuracy macro avg weighted avg precision 0.94 0.93 0.93 0.93 0.93 recall 0.92 0.95 0.93 0.93 0.93 f1-score 0.93 0.94 0.93 0.93 0.93 support 97.00 115.00 0.93 212.00 212.00 _______________________________________________ Confusion Matrix: [[ 89 8] [ 6 109]] Test Result: ================================================ Accuracy Score: 87.91% _______________________________________________ CLASSIFICATION REPORT: 0 1 accuracy macro avg weighted avg precision 0.86 0.90 0.88 0.88 0.88 recall 0.88 0.88 0.88 0.88 0.88 f1-score 0.87 0.89 0.88 0.88 0.88 support 41.00 50.00 0.88 91.00 91.00 _______________________________________________ Confusion Matrix: [[36 5] [ 6 44]] . test_score = accuracy_score(y_test, svm_clf.predict(X_test)) * 100 train_score = accuracy_score(y_train, svm_clf.predict(X_train)) * 100 results_df_2 = pd.DataFrame(data=[[&quot;Support Vector Machine&quot;, train_score, test_score]], columns=[&#39;Model&#39;, &#39;Training Accuracy %&#39;, &#39;Testing Accuracy %&#39;]) results_df = results_df.append(results_df_2, ignore_index=True) results_df . Model Training Accuracy % Testing Accuracy % . 0 Logistic Regression | 86.79 | 86.81 | . 1 K-nearest neighbors | 86.79 | 86.81 | . 2 Support Vector Machine | 93.40 | 87.91 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Model 4: Decision Tree Classifier . from sklearn.tree import DecisionTreeClassifier tree_clf = DecisionTreeClassifier(random_state=42) tree_clf.fit(X_train, y_train) print_score(tree_clf, X_train, y_train, X_test, y_test, train=True) print_score(tree_clf, X_train, y_train, X_test, y_test, train=False) . Train Result: ================================================ Accuracy Score: 100.00% _______________________________________________ CLASSIFICATION REPORT: 0 1 accuracy macro avg weighted avg precision 1.00 1.00 1.00 1.00 1.00 recall 1.00 1.00 1.00 1.00 1.00 f1-score 1.00 1.00 1.00 1.00 1.00 support 97.00 115.00 1.00 212.00 212.00 _______________________________________________ Confusion Matrix: [[ 97 0] [ 0 115]] Test Result: ================================================ Accuracy Score: 78.02% _______________________________________________ CLASSIFICATION REPORT: 0 1 accuracy macro avg weighted avg precision 0.72 0.84 0.78 0.78 0.79 recall 0.83 0.74 0.78 0.78 0.78 f1-score 0.77 0.79 0.78 0.78 0.78 support 41.00 50.00 0.78 91.00 91.00 _______________________________________________ Confusion Matrix: [[34 7] [13 37]] . test_score = accuracy_score(y_test, tree_clf.predict(X_test)) * 100 train_score = accuracy_score(y_train, tree_clf.predict(X_train)) * 100 results_df_2 = pd.DataFrame(data=[[&quot;Decision Tree Classifier&quot;, train_score, test_score]], columns=[&#39;Model&#39;, &#39;Training Accuracy %&#39;, &#39;Testing Accuracy %&#39;]) results_df = results_df.append(results_df_2, ignore_index=True) results_df . Model Training Accuracy % Testing Accuracy % . 0 Logistic Regression | 86.79 | 86.81 | . 1 K-nearest neighbors | 86.79 | 86.81 | . 2 Support Vector Machine | 93.40 | 87.91 | . 3 Decision Tree Classifier | 100.00 | 78.02 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Model 5: Random Forest . from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import RandomizedSearchCV rf_clf = RandomForestClassifier(n_estimators=1000, random_state=42) rf_clf.fit(X_train, y_train) print_score(rf_clf, X_train, y_train, X_test, y_test, train=True) print_score(rf_clf, X_train, y_train, X_test, y_test, train=False) . Train Result: ================================================ Accuracy Score: 100.00% _______________________________________________ CLASSIFICATION REPORT: 0 1 accuracy macro avg weighted avg precision 1.00 1.00 1.00 1.00 1.00 recall 1.00 1.00 1.00 1.00 1.00 f1-score 1.00 1.00 1.00 1.00 1.00 support 97.00 115.00 1.00 212.00 212.00 _______________________________________________ Confusion Matrix: [[ 97 0] [ 0 115]] Test Result: ================================================ Accuracy Score: 82.42% _______________________________________________ CLASSIFICATION REPORT: 0 1 accuracy macro avg weighted avg precision 0.80 0.84 0.82 0.82 0.82 recall 0.80 0.84 0.82 0.82 0.82 f1-score 0.80 0.84 0.82 0.82 0.82 support 41.00 50.00 0.82 91.00 91.00 _______________________________________________ Confusion Matrix: [[33 8] [ 8 42]] . test_score = accuracy_score(y_test, rf_clf.predict(X_test)) * 100 train_score = accuracy_score(y_train, rf_clf.predict(X_train)) * 100 results_df_2 = pd.DataFrame(data=[[&quot;Random Forest Classifier&quot;, train_score, test_score]], columns=[&#39;Model&#39;, &#39;Training Accuracy %&#39;, &#39;Testing Accuracy %&#39;]) results_df = results_df.append(results_df_2, ignore_index=True) results_df . Model Training Accuracy % Testing Accuracy % . 0 Logistic Regression | 86.79 | 86.81 | . 1 K-nearest neighbors | 86.79 | 86.81 | . 2 Support Vector Machine | 93.40 | 87.91 | . 3 Decision Tree Classifier | 100.00 | 78.02 | . 4 Random Forest Classifier | 100.00 | 82.42 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Conclusions . The next step would be to explore hyperparameter tunings for the various ML models . From the preliminary data, though, it seems that Logistic Regression and Support Vector Machine models give the highest % testing accuracy, predicting heart disease from the features with a &gt; 86% accuracy. .",
            "url": "https://pkalnins.github.io/ws/data%20visualization/machine%20learning/2022/08/10/HeartDisease.html",
            "relUrl": "/data%20visualization/machine%20learning/2022/08/10/HeartDisease.html",
            "date": " • Aug 10, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://pkalnins.github.io/ws/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://pkalnins.github.io/ws/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I’m a Naturoapathic physician and former professor, currently training to be a data scientist .",
          "url": "https://pkalnins.github.io/ws/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://pkalnins.github.io/ws/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}